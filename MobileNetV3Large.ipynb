{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic\n",
    "import os\n",
    "from os import makedirs\n",
    "from os import listdir\n",
    "from shutil import copyfile\n",
    "from random import seed\n",
    "from random import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "\n",
    "# visuals\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "from PIL import Image\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix,ConfusionMatrixDisplay\n",
    "\n",
    "datasetzipfile = \"datasets.zip\"\n",
    "\n",
    "# Check if the zip file exists\n",
    "if os.path.exists(datasetzipfile):\n",
    "    # Open the zip file in read mode\n",
    "    with zipfile.ZipFile(datasetzipfile, 'r') as zip_ref:\n",
    "        # Iterate through each file in the zip archive\n",
    "        for file_info in zip_ref.infolist():\n",
    "            # Check if the file already exists in the current directory\n",
    "            if not os.path.exists(file_info.filename):\n",
    "                zip_ref.extract(file_info)\n",
    "else:\n",
    "    print(f\"{datasetzipfile} does not exist.\")\n",
    "\n",
    "train_path_dog = \"datasets/train/dog\"\n",
    "train_path_cat = \"datasets/train/cat\"\n",
    "valid_path_dog = \"datasets/val/dog\"\n",
    "valid_path_cat = \"datasets/val/cat\"\n",
    "\n",
    "test_path = \"datasets/test\"\n",
    "\n",
    "#Count the Data Provided\n",
    "def count_files_in_directory(path):\n",
    "    try:\n",
    "        # List all entries in the specified directory\n",
    "        all_entries = os.listdir(path)\n",
    "        \n",
    "        # Count only the files (exclude directories)\n",
    "        total_files = sum(1 for entry in all_entries if os.path.isfile(os.path.join(path, entry)))\n",
    "        \n",
    "        return total_files\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "# Specify the directory path\n",
    "print(\"Total number of dog images in training data :\", count_files_in_directory(train_path_dog))\n",
    "print(\"Total number of cat images in training data :\", count_files_in_directory(train_path_cat))\n",
    "print()\n",
    "print(\"Total number of dog images in validation data :\", count_files_in_directory(valid_path_dog))\n",
    "print(\"Total number of cat images in validation data :\", count_files_in_directory(valid_path_cat))\n",
    "print()\n",
    "print(\"Total number of unknown images in test data :\", count_files_in_directory(test_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import MobileNetV3Large\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# Paths to training and validation directories\n",
    "train_path = \"datasets/train\"\n",
    "valid_path = \"datasets/val\"\n",
    "\n",
    "# Data augmentation for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Data generators for loading images from directories\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_path,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "valid_generator = valid_datagen.flow_from_directory(\n",
    "    valid_path,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "# Function to create the model\n",
    "def create_model():\n",
    "    \"\"\"Build and compile the MobileNetV3Large model.\"\"\"\n",
    "    # Load MobileNetV3Large with ImageNet weights\n",
    "    base_model = MobileNetV3Large(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "    # Freeze the base model's layers\n",
    "    base_model.trainable = False\n",
    "\n",
    "    # Build the model\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    print(\"Model Summary\")\n",
    "    print(\"--------------\")\n",
    "    print(\"CNN Model: MobileNetV3Large\")\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "# Callbacks for learning rate reduction and early stopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.0001)\n",
    "\n",
    "print()\n",
    "print(\"Data generators are created.\")\n",
    "print(\"Early stopping and learning rate reduction callbacks are set.\")\n",
    "print()\n",
    "\n",
    "# Function to build your model\n",
    "model = create_model()  \n",
    "\n",
    "# Save the model architecture as an image\n",
    "plot_model(model, to_file='model_architecture.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "# Display the model architecture\n",
    "Image.open('model_architecture.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_runs = 1\n",
    "val_acc_results = []\n",
    "val_loss_results = []\n",
    "\n",
    "for run in range(n_runs):\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(train_generator, \n",
    "                        validation_data=valid_generator,\n",
    "                        epochs=25,\n",
    "                        callbacks=[early_stop, reduce_lr])\n",
    "    \n",
    "    # Evaluate and store the validation accuracy\n",
    "    val_loss, val_acc = model.evaluate(valid_generator)\n",
    "    val_acc_results.append(val_acc)\n",
    "    val_loss_results.append(val_loss)\n",
    "\n",
    "    print(\"Run \" + run + \" Results:\")\n",
    "    print(f\"Validation accuracy: {val_acc:.4f}\")\n",
    "    print(f\"Validation loss: {val_loss:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print average results\n",
    "average_val_acc = np.mean(val_acc_results)\n",
    "average_val_loss = np.mean(val_loss_results)\n",
    "\n",
    "print(\"Average Results:\")\n",
    "print(f\"Average Validation Accuracy: {average_val_acc:.4f}\")\n",
    "print(f\"Average Validation Loss: {average_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to hold the results\n",
    "results = []\n",
    "\n",
    "# Loop through each file in the test directory\n",
    "for img_file in os.listdir(test_path):\n",
    "    # Load and preprocess the image\n",
    "    img_path = os.path.join(test_path, img_file)\n",
    "    img = image.load_img(img_path, target_size=(224, 224))  # Size should match input size of the model\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "    img_array /= 255.0  # Rescale to match training preprocessing\n",
    "\n",
    "    # Make a prediction\n",
    "    prediction = model.predict(img_array)\n",
    "    # If prediction >= 0.5, it's a dog (1); otherwise, it's a cat (0)\n",
    "    prediction_label = 1 if prediction >= 0.5 else 0\n",
    "    \n",
    "    # Append the filename and prediction to the results list\n",
    "    results.append([os.path.splitext(img_file)[0], prediction_label])\n",
    "\n",
    "# Create a DataFrame for the results\n",
    "df = pd.DataFrame(results, columns=['id', 'label'])\n",
    "\n",
    "# Save the results to a CSV file\n",
    "df.to_csv('submission.csv', index=False)\n",
    "print(\"Predictions saved to submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
