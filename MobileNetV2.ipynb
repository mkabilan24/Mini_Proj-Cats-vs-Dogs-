{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Libraries and Extracting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic\n",
    "import os\n",
    "from os import makedirs\n",
    "from os import listdir\n",
    "from shutil import copyfile\n",
    "from random import seed\n",
    "from random import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "\n",
    "# visuals\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "from PIL import Image\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix,ConfusionMatrixDisplay\n",
    "\n",
    "datasetzipfile = \"datasets.zip\"\n",
    "\n",
    "# Check if the zip file exists\n",
    "if os.path.exists(datasetzipfile):\n",
    "    # Open the zip file in read mode\n",
    "    with zipfile.ZipFile(datasetzipfile, 'r') as zip_ref:\n",
    "        # Iterate through each file in the zip archive\n",
    "        for file_info in zip_ref.infolist():\n",
    "            # Check if the file already exists in the current directory\n",
    "            if not os.path.exists(file_info.filename):\n",
    "                zip_ref.extract(file_info)\n",
    "else:\n",
    "    print(f\"{datasetzipfile} does not exist.\")\n",
    "\n",
    "train_path_dog = \"datasets/train/dog\"\n",
    "train_path_cat = \"datasets/train/cat\"\n",
    "valid_path_dog = \"datasets/val/dog\"\n",
    "valid_path_cat = \"datasets/val/cat\"\n",
    "\n",
    "test_path = \"datasets/test\"\n",
    "\n",
    "#Count the Data Provided\n",
    "def count_files_in_directory(path):\n",
    "    try:\n",
    "        # List all entries in the specified directory\n",
    "        all_entries = os.listdir(path)\n",
    "        \n",
    "        # Count only the files (exclude directories)\n",
    "        total_files = sum(1 for entry in all_entries if os.path.isfile(os.path.join(path, entry)))\n",
    "        \n",
    "        return total_files\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "# Specify the directory path\n",
    "print(\"Total number of dog images in training data :\", count_files_in_directory(train_path_dog))\n",
    "print(\"Total number of cat images in training data :\", count_files_in_directory(train_path_cat))\n",
    "print()\n",
    "print(\"Total number of dog images in validation data :\", count_files_in_directory(valid_path_dog))\n",
    "print(\"Total number of cat images in validation data :\", count_files_in_directory(valid_path_cat))\n",
    "print()\n",
    "print(\"Total number of unknown images in test data :\", count_files_in_directory(test_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualise Images of Dogs from Train Dataset\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.subplots_adjust(hspace=0.4)\n",
    "\n",
    "# Initialize a counter for the number of images plotted\n",
    "count = 0\n",
    "\n",
    "for i in range(10):\n",
    "    filename = f'{train_path_dog}/dog.{i}.jpg'\n",
    "    \n",
    "    # Check if the file exists\n",
    "    if os.path.exists(filename):\n",
    "        plt.subplot(1, 10, count + 1)  # Create a subplot for the existing image\n",
    "        image = imread(filename)\n",
    "        plt.imshow(image)\n",
    "        plt.title('Dog' + str(i), fontsize=12)\n",
    "        plt.axis('off')\n",
    "        \n",
    "        count += 1  # Increment the counter for each plotted image\n",
    "\n",
    "# Adjust the layout to accommodate the images\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualise Images of Cats from Train Dataset\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.subplots_adjust(hspace=0.4)\n",
    "\n",
    "# Initialize a counter for the number of images plotted\n",
    "count = 0\n",
    "\n",
    "for i in range(10):\n",
    "    filename = f'{train_path_cat}/cat.{i}.jpg'\n",
    "    \n",
    "    # Check if the file exists\n",
    "    if os.path.exists(filename):\n",
    "        plt.subplot(1, 10, count + 1)  # Create a subplot for the existing image\n",
    "        image = imread(filename)\n",
    "        plt.imshow(image)\n",
    "        plt.title('Cat' + str(i), fontsize=12)\n",
    "        plt.axis('off')\n",
    "        \n",
    "        count += 1  # Increment the counter for each plotted image\n",
    "\n",
    "# Adjust the layout to accommodate the images\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MobileNetV2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# Paths to training and validation directories\n",
    "train_path = \"datasets/train\"\n",
    "valid_path = \"datasets/val\"\n",
    "\n",
    "# Data augmentation for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Data generators for loading images from directories\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_path,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "valid_generator = valid_datagen.flow_from_directory(\n",
    "    valid_path,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "# Function to create the model\n",
    "\"\"\"Build and compile the MobileNetV2 model.\"\"\"\n",
    "# Load MobileNetV3Large with ImageNet weights\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the base model's layers\n",
    "base_model.trainable = False\n",
    "\n",
    "# Build the model\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"Model Summary\")\n",
    "print(\"--------------\")\n",
    "print(\"CNN Model: MobileNetV2\")\n",
    "print(model.summary())\n",
    "\n",
    "# Callbacks for learning rate reduction and early stopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.0001)\n",
    "\n",
    "print()\n",
    "print(\"Data generators are created.\")\n",
    "print(\"Early stopping and learning rate reduction callbacks are set.\")\n",
    "print()\n",
    "\n",
    "# Save the model architecture as an image\n",
    "plot_model(model, to_file='model_architecture.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "# Display the model architecture\n",
    "Image.open('model_architecture.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of runs\n",
    "n_runs = 5\n",
    "val_acc_results = []\n",
    "val_loss_results = []\n",
    "\n",
    "for run in range(n_runs):\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(train_generator, \n",
    "                        validation_data=valid_generator,\n",
    "                        epochs=25,\n",
    "                        callbacks=[early_stop, reduce_lr])\n",
    "    \n",
    "    # Evaluate and store the validation accuracy\n",
    "    val_loss, val_acc = model.evaluate(valid_generator)\n",
    "    val_acc_results.append(val_acc)\n",
    "    val_loss_results.append(val_loss)\n",
    "\n",
    "    print(\"Run \" + str(run+1) + \" Results:\")\n",
    "    print(f\"Validation accuracy: {val_acc:.4f}\")\n",
    "    print(f\"Validation loss: {val_loss:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Accuracy and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print average results\n",
    "average_val_acc = np.mean(val_acc_results)\n",
    "average_val_loss = np.mean(val_loss_results)\n",
    "\n",
    "print(\"Average Results:\")\n",
    "print(f\"Average Validation Accuracy: {average_val_acc:.4f}\")\n",
    "print(f\"Average Validation Loss: {average_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to hold the results\n",
    "results = []\n",
    "\n",
    "# Loop through each file in the test directory\n",
    "for img_file in os.listdir(test_path):\n",
    "    # Load and preprocess the image\n",
    "    img_path = os.path.join(test_path, img_file)\n",
    "    img = image.load_img(img_path, target_size=(224, 224))  # Size should match input size of the model\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "    img_array /= 255.0  # Rescale to match training preprocessing\n",
    "\n",
    "    # Make a prediction\n",
    "    prediction = model.predict(img_array)\n",
    "    # If prediction >= 0.5, it's a dog (1); otherwise, it's a cat (0)\n",
    "    prediction_label = 1 if prediction >= 0.5 else 0\n",
    "    \n",
    "    # Append the filename and prediction to the results list\n",
    "    results.append([os.path.splitext(img_file)[0], prediction_label])\n",
    "\n",
    "# Create a DataFrame for the results\n",
    "df = pd.DataFrame(results, columns=['id', 'label'])\n",
    "\n",
    "# Save the results to a CSV file\n",
    "df.to_csv('submission.csv', index=False)\n",
    "print(\"Predictions saved to submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting CIFAR-10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Path to the CIFAR-10 tar file\n",
    "tar_file_path = 'cifar-10-python.tar'\n",
    "\n",
    "# Extract the tar file\n",
    "with tarfile.open(tar_file_path, 'r:') as tar:\n",
    "    tar.extractall()\n",
    "    print(\"Extraction completed.\")\n",
    "\n",
    "# Path to the extracted CIFAR-10 data\n",
    "cifar10_data_path = './cifar-10-batches-py/'\n",
    "\n",
    "# Function to load a batch of CIFAR-10 data\n",
    "def load_cifar_batch(filename):\n",
    "    with open(filename, 'rb') as file:\n",
    "        batch = pickle.load(file, encoding='latin1')\n",
    "        images = batch['data']\n",
    "        labels = batch['labels']\n",
    "        # CIFAR-10 images are 32x32 with 3 channels\n",
    "        images = images.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "        return images, labels\n",
    "\n",
    "# Load a batch of CIFAR-10 data for viewing\n",
    "batch_file = os.path.join(cifar10_data_path, 'data_batch_1')\n",
    "images, labels = load_cifar_batch(batch_file)\n",
    "\n",
    "# CIFAR-10 classes\n",
    "cifar10_classes = [\n",
    "    'airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'\n",
    "]\n",
    "\n",
    "# Display some sample images with their labels\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(9):\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(images[i])\n",
    "    plt.title(cifar10_classes[labels[i]])\n",
    "    plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modified MobileNetV2 for CIFAR-10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Path to CIFAR-10 tar file\n",
    "tar_file_path = 'cifar-10-python.tar'\n",
    "\n",
    "# Extract the tar file\n",
    "with tarfile.open(tar_file_path, 'r:') as tar:\n",
    "    tar.extractall()\n",
    "    print(\"Extraction completed.\")\n",
    "\n",
    "# Path to extracted CIFAR-10 data\n",
    "cifar10_data_path = './cifar-10-batches-py/'\n",
    "\n",
    "# Load CIFAR-10 batches\n",
    "def load_cifar_data(data_path):\n",
    "    images, labels = [], []\n",
    "    for i in range(1, 6):  # CIFAR-10 has 5 training batches\n",
    "        with open(f\"{data_path}data_batch_{i}\", 'rb') as file:\n",
    "            batch = pickle.load(file, encoding='latin1')\n",
    "            images.append(batch['data'])\n",
    "            labels.extend(batch['labels'])\n",
    "    images = np.concatenate(images)\n",
    "    images = images.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)  # Reshape to (32, 32, 3)\n",
    "    return images, np.array(labels)\n",
    "\n",
    "# Load training and test data\n",
    "X_train, y_train = load_cifar_data(cifar10_data_path)\n",
    "with open(f\"{cifar10_data_path}test_batch\", 'rb') as file:\n",
    "    test_batch = pickle.load(file, encoding='latin1')\n",
    "    X_test = test_batch['data'].reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "    y_test = np.array(test_batch['labels'])\n",
    "\n",
    "# Normalize pixel values\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "\n",
    "# Convert labels to categorical\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Split training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "# Build MobileNetV2 model\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
    "base_model.trainable = False\n",
    "\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(10, activation='softmax')  # Output layer for 10 classes\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks for learning rate reduction and early stopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.0001)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop, reduce_lr]\n",
    ")\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test accuracy: {test_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Path to CIFAR-10 tar file\n",
    "tar_file_path = 'cifar-10-python.tar'\n",
    "\n",
    "# Extract the tar file\n",
    "with tarfile.open(tar_file_path, 'r:') as tar:\n",
    "    tar.extractall()\n",
    "    print(\"Extraction completed.\")\n",
    "\n",
    "# Path to extracted CIFAR-10 data\n",
    "cifar10_data_path = './cifar-10-batches-py/'\n",
    "\n",
    "# Load CIFAR-10 batches\n",
    "def load_cifar_data(data_path):\n",
    "    images, labels = [], []\n",
    "    for i in range(1, 6):  # CIFAR-10 has 5 training batches\n",
    "        with open(f\"{data_path}data_batch_{i}\", 'rb') as file:\n",
    "            batch = pickle.load(file, encoding='latin1')\n",
    "            images.append(batch['data'])\n",
    "            labels.extend(batch['labels'])\n",
    "    images = np.concatenate(images)\n",
    "    images = images.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)  # Reshape to (32, 32, 3)\n",
    "    return images, np.array(labels)\n",
    "\n",
    "# Load training and test data\n",
    "X_train, y_train = load_cifar_data(cifar10_data_path)\n",
    "with open(f\"{cifar10_data_path}test_batch\", 'rb') as file:\n",
    "    test_batch = pickle.load(file, encoding='latin1')\n",
    "    X_test = test_batch['data'].reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "    y_test = np.array(test_batch['labels'])\n",
    "\n",
    "# Normalize pixel values\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "\n",
    "# Convert labels to categorical\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Split training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "# Resize images to a higher resolution for MobileNetV2\n",
    "X_train = tf.image.resize(X_train, (96, 96))\n",
    "X_val = tf.image.resize(X_val, (96, 96))\n",
    "X_test = tf.image.resize(X_test, (96, 96))\n",
    "\n",
    "# Data Augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    rotation_range=15,\n",
    "    zoom_range=0.1\n",
    ")\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# Build MobileNetV2 model\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(96, 96, 3))\n",
    "base_model.trainable = True\n",
    "\n",
    "# Fine-tune by freezing early layers\n",
    "for layer in base_model.layers[:-20]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Model architecture\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(10, activation='softmax')  # Output layer for 10 classes\n",
    "])\n",
    "\n",
    "# Use label smoothing in loss\n",
    "loss = tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1)\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001, decay=1e-5),\n",
    "              loss=loss, metrics=['accuracy'])\n",
    "\n",
    "# Callbacks for learning rate reduction and early stopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.0001)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    datagen.flow(X_train, y_train, batch_size=32),\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,\n",
    "    callbacks=[early_stop, reduce_lr]\n",
    ")\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test accuracy: {test_accuracy:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
